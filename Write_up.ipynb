{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vehicle Detection Project\n",
    "\n",
    "## The goals / steps of this project are the following:\n",
    "\n",
    "-Perform a Histogram of Oriented Gradients (HOG) feature extraction on a labeled training set of images and train a classifier Linear SVM classifier\n",
    "\n",
    "-Optionally, apply a color transform and append binned color features, as well as histograms of color, to HOG feature vector.\n",
    "\n",
    "-Note: for those first two steps don't forget to normalize the features and randomize a selection for training and testing.\n",
    "\n",
    "-Implement a sliding-window technique and use the trained classifier to search for vehicles in images.\n",
    "\n",
    "-Run the pipeline on a video stream (start with the test_video.mp4 and later implement on full project_video.mp4) and create a heat map of recurring detections frame by frame to reject outliers and follow detected vehicles.\n",
    "\n",
    "-Estimate a bounding box for vehicles detected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram of Oriented Gradients (HOG)\n",
    "\n",
    "#### 1. Explain how to extract HOG features from the training images.\n",
    "\n",
    "Using HOG function from skimage.feature to extraction HOG feature. And also extract color histogram and spatially binned color feature from the training images.\n",
    "\n",
    "I used get_hog_features() function defined in the function section of the Trial_final_3.ipynb 3rd cell and executed in the 4th cell of the same document. Before doing that I created two different lists. One for car images and the other for notcar images. This helped in reading different images of the same class and then converting them into gray scale which in turn helped in getting the HOG features. The code for this is present in cell 4 of the Trial_final_3.ipynb and their respective functions in the previous cell. The code snippet is as follows:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "orient = 9 # HOG orientations\n",
    "pix_per_cell = 8 # HOG pixels per cell\n",
    "cell_per_block = 2 # HOG cells per block\n",
    "# Choose random car/not-car indices\n",
    "car_ind = np.random.randint(0, len(cars))\n",
    "notcar_ind = np.random.randint(0, len(notcars))\n",
    "\n",
    "# Read in car/not-car images\n",
    "car_image = mpimg.imread(cars[car_ind])\n",
    "notcar_image = mpimg.imread(notcars[notcar_ind])\n",
    "\n",
    "# car = mpimg.imread(cars[4])\n",
    "# notcar = mpimg.imread(notcars[4])\n",
    "gray_car = cv2.cvtColor(car_image, cv2.COLOR_RGB2GRAY)\n",
    "gray_notcar = cv2.cvtColor(notcar_image, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "car_feature, car_hog = get_hog_features(gray_car, orient, pix_per_cell, cell_per_block,\n",
    "                        vis=True, feature_vec=True)\n",
    "notcar_feature, notcar_hog = get_hog_features(gray_notcar, orient, pix_per_cell, cell_per_block,\n",
    "                        vis=True, feature_vec=True)\n",
    "fig = plt.figure(figsize=(18,14))\n",
    "plt.subplot(151)\n",
    "plt.imshow(car_image)\n",
    "plt.title('Example Car Image')\n",
    "plt.subplot(152)\n",
    "plt.imshow(car_image[:,:,0], cmap='gray')\n",
    "plt.title('Example Car Image CH_1')\n",
    "plt.subplot(153)\n",
    "plt.imshow(car_image[:,:,1], cmap='gray')\n",
    "plt.title('Example Car Image CH_2')\n",
    "plt.subplot(154)\n",
    "plt.imshow(car_image[:,:,2], cmap='gray')\n",
    "plt.title('Example Car Image CH_3')\n",
    "plt.subplot(155)\n",
    "plt.imshow(car_hog, cmap='hot')\n",
    "plt.title('Car HOG Visualization')\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(figsize=(18,14))\n",
    "plt.subplot(251)\n",
    "plt.imshow(notcar_image)\n",
    "plt.title('Example Not Car Image')\n",
    "plt.subplot(252)\n",
    "plt.imshow(notcar_image[:,:,0], cmap='gray')\n",
    "plt.title('Example Not Car Image CH_1')\n",
    "plt.subplot(253)\n",
    "plt.imshow(notcar_image[:,:,1], cmap='gray')\n",
    "plt.title('Example Not Car Image CH_2')\n",
    "plt.subplot(254)\n",
    "plt.imshow(notcar_image[:,:,2], cmap='gray')\n",
    "plt.title('Example Not Car Image CH_3')\n",
    "plt.subplot(255)\n",
    "plt.imshow(notcar_hog, cmap='hot')\n",
    "plt.title('Car HOG Visualization')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"output_images/1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"output_images/2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also explored different channels for visualizing how the images are as seen above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Explain how you settled on your final choice of HOG parameters.\n",
    "\n",
    "I tried these parameters to tune:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Define feature parameters\n",
    "color_space = 'YCrCb' # Can be RGB, HSV, LUV, HLS, YUV, YCrCb\n",
    "orient = 9  # HOG orientations\n",
    "pix_per_cell = 8 # HOG pixels per cell\n",
    "cell_per_block = 2 # HOG cells per block\n",
    "hog_channel = 'ALL' # Can be 0, 1, 2, or \"ALL\"\n",
    "spatial_size = (32, 32) # Spatial binning dimensions\n",
    "hist_bins = 32    # Number of histogram bins\n",
    "spatial_feat = True # Spatial features on or off\n",
    "hist_feat = True # Histogram features on or off\n",
    "hog_feat = True # HOG features on or off\n",
    "#y_start_stop = [None, None] # Min and max in y to search in slide_window()\n",
    "t = time.time()\n",
    "n_samples = 1000\n",
    "random_idxs = np.random.randint(0, len(cars), n_samples)\n",
    "test_cars = cars #np.array(cars)[random_idxs]\n",
    "test_notcars = notcars #np.array(notcars)[random_idxs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These parameters are the utmost important ones. Tuning these can make a model or destroy a model. The RGB color space doesn't help in improving the model so after trial and errors I settled for YCrCb space. However, HSV and LUV proved to be good but under some bumps it faced problems in detecting. I took all the HOG channels as it covered as many features to train on as we can. spatial_size and hist_bins I have taken it as 32 as that number better results for my data and architecture. I decided to go with 1000 samples as it reduced the amout of time to train and could check the results quickly for tweaking the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Describe how (and identify where in your code) you trained a classifier using your selected HOG features (and color features if you used them).\n",
    "\n",
    "I trained a linear SVM using LinearSVC from sklearn.svm to classify cars and notcars. The code snippet can be found in the Trial_final_3.ipynb in the 4th cell with respective functions in the 3rd cell. The code is also present below for the immediate reference:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "car_features = extract_features(test_cars, color_space=color_space,\n",
    "                        spatial_size=spatial_size, hist_bins=hist_bins,\n",
    "                        orient=orient, pix_per_cell=pix_per_cell,\n",
    "                        cell_per_block=cell_per_block,\n",
    "                        hog_channel=hog_channel, spatial_feat=spatial_feat,\n",
    "                        hist_feat=hist_feat, hog_feat=hog_feat)\n",
    "notcar_features = extract_features(test_notcars, color_space=color_space,\n",
    "                        spatial_size=spatial_size, hist_bins=hist_bins,\n",
    "                        orient=orient, pix_per_cell=pix_per_cell,\n",
    "                        cell_per_block=cell_per_block,\n",
    "                        hog_channel=hog_channel, spatial_feat=spatial_feat,\n",
    "                        hist_feat=hist_feat, hog_feat=hog_feat)\n",
    "\n",
    "X = np.vstack((car_features, notcar_features)).astype(np.float64)\n",
    "# Fit a per-column scaler\n",
    "X_scaler = StandardScaler().fit(X)\n",
    "# Apply the scaler to X\n",
    "scaled_X = X_scaler.transform(X)\n",
    "\n",
    "# Define the labels vector\n",
    "y = np.hstack((np.ones(len(car_features)), np.zeros(len(notcar_features))))\n",
    "\n",
    "\n",
    "# Split up data into randomized training and test sets\n",
    "rand_state = np.random.randint(0, 100)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    scaled_X, y, test_size=0.2, random_state=rand_state)\n",
    "\n",
    "print('Using:',orient,'orientations',pix_per_cell,\n",
    "    'pixels per cell and', cell_per_block,'cells per block')\n",
    "print('Feature vector length:', len(X_train[0]))\n",
    "# Use a linear SVC\n",
    "svc = LinearSVC()\n",
    "# Check the training time for the SVC\n",
    "t=time.time()\n",
    "svc.fit(X_train, y_train)\n",
    "t2 = time.time()\n",
    "print(round(t2-t, 2), 'Seconds to train SVC...')\n",
    "# Check the score of the SVC\n",
    "print('Test Accuracy of SVC = ', round(svc.score(X_test, y_test), 4))\n",
    "# Check the prediction time for a single sample\n",
    "t=time.time()\n",
    "n_predict = 10\n",
    "print('My SVC predicts: ', svc.predict(X_test[0:n_predict]))\n",
    "print('For these',n_predict, 'labels: ', y_test[0:n_predict])\n",
    "t2 = time.time()\n",
    "print(round(t2-t, 5), 'Seconds to predict', n_predict,'labels with SVC')\n",
    "\n",
    "svc, X_scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To tackle with the overfitting the data was split with test size of 20% and the training data with 80%. To extract features for the classification extract_features function was defined in the fucntion section (3rd cell of the Tial_final_3.ipynb). I used all the HOG channel and the YCrCb color_space to identify the objects clearly. The svc and the X_scalar were pickl dumped so that it can be used later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sliding Window Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Describe how (and identify where in your code) you implemented a sliding window search. How did you decide what scales to search and how much to overlap windows?\n",
    "\n",
    "To implement sliding window search I used find_cars method dicussed in the course. Snippet of the code is in the 9th cell of the same ipynb file. A snippet of that code is also give here:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def find_cars(img, ystart, ystop, scale, svc, X_scaler, orient, pix_per_cell, cell_per_block, spatial_size, hist_bins):\n",
    "    \n",
    "    draw_img = np.copy(img)\n",
    "    img = img.astype(np.float32)/255\n",
    "    heatmap = np.zeros_like(img[:,:,0])\n",
    "    y_top = int(img.shape[0]//2)\n",
    "    \n",
    "    img_tosearch = img[ystart:ystop,:,:]\n",
    "    ctrans_tosearch = convert_color(img_tosearch, conv='RGB2YCrCb')\n",
    "    if scale != 1:\n",
    "        imshape = ctrans_tosearch.shape\n",
    "        ctrans_tosearch = cv2.resize(ctrans_tosearch, (np.int(imshape[1]/scale), np.int(imshape[0]/scale)))\n",
    "        \n",
    "    ch1 = ctrans_tosearch[:,:,0]\n",
    "    ch2 = ctrans_tosearch[:,:,1]\n",
    "    ch3 = ctrans_tosearch[:,:,2]\n",
    "\n",
    "    # Define blocks and steps as above\n",
    "    nxblocks = (ch1.shape[1] // pix_per_cell)-1\n",
    "    nyblocks = (ch1.shape[0] // pix_per_cell)-1 \n",
    "    nfeat_per_block = orient*cell_per_block**2\n",
    "    # 64 was the orginal sampling rate, with 8 cells and 8 pix per cell\n",
    "    window = 64\n",
    "    nblocks_per_window = (window // pix_per_cell)-1 \n",
    "    cells_per_step = 2  # Instead of overlap, define how many cells to step\n",
    "    nxsteps = (nxblocks - nblocks_per_window) // cells_per_step\n",
    "    nysteps = (nyblocks - nblocks_per_window) // cells_per_step\n",
    "    \n",
    "    # Compute individual channel HOG features for the entire image\n",
    "    hog1 = get_hog_features(ch1, orient, pix_per_cell, cell_per_block, feature_vec=False)\n",
    "    hog2 = get_hog_features(ch2, orient, pix_per_cell, cell_per_block, feature_vec=False)\n",
    "    hog3 = get_hog_features(ch3, orient, pix_per_cell, cell_per_block, feature_vec=False)\n",
    "    window_list = []\n",
    "    for xb in range(nxsteps):\n",
    "        for yb in range(nysteps):\n",
    "            ypos = yb*cells_per_step\n",
    "            xpos = xb*cells_per_step\n",
    "            # Extract HOG for this patch\n",
    "            hog_feat1 = hog1[ypos:ypos+nblocks_per_window, xpos:xpos+nblocks_per_window].ravel() \n",
    "            hog_feat2 = hog2[ypos:ypos+nblocks_per_window, xpos:xpos+nblocks_per_window].ravel() \n",
    "            hog_feat3 = hog3[ypos:ypos+nblocks_per_window, xpos:xpos+nblocks_per_window].ravel() \n",
    "            hog_features = np.hstack((hog_feat1, hog_feat2, hog_feat3))\n",
    "\n",
    "            xleft = xpos*pix_per_cell\n",
    "            ytop = ypos*pix_per_cell\n",
    "\n",
    "            # Extract the image patch\n",
    "            subimg = cv2.resize(ctrans_tosearch[ytop:ytop+window, xleft:xleft+window], (64,64))\n",
    "          \n",
    "            # Get color features\n",
    "            spatial_features = bin_spatial(subimg, size=spatial_size)\n",
    "            hist_features = color_hist(subimg, nbins=hist_bins)\n",
    "\n",
    "            # Scale features and make a prediction\n",
    "            test_features = X_scaler.transform(np.hstack((spatial_features, hist_features, hog_features)).reshape(1, -1))    \n",
    "            #test_features = X_scaler.transform(np.hstack((shape_feat, hist_feat)).reshape(1, -1))    \n",
    "            test_prediction = svc.predict(test_features)\n",
    "            \n",
    "            if test_prediction == 1:\n",
    "                xbox_left = np.int(xleft*scale)\n",
    "                ytop_draw = np.int(ytop*scale)\n",
    "                win_draw = np.int(window*scale)\n",
    "                box = ((xbox_left, ytop_draw+ystart),(xbox_left+win_draw,ytop_draw+win_draw+ystart))\n",
    "                window_list.append(box)\n",
    "                cv2.rectangle(draw_img,box[0],box[1],(0,0,255),6) \n",
    "                heatmap[box[0][1]:box[1][1], box[0][0]:box[1][0]] += 1                \n",
    "    \n",
    "    return draw_img, heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially I started working with a different approach of sliding window wich did give good results but it was good for the static images. For video purposes I had work around few parameters. Then I came to this method which was discussed in the recent course update. I limited the search area as doing that reduced the time to search and track immensely. I used to 3 to 4 scales to cover all the possible places a car can be detected but that resulted in very very unstable detection with some false positives then I slowly started experimenting with different individual scales and combination which at last fruited into deciding which scale works better for my model and it was the standard scale of 1.5. I also added the heatmap variable and let is be mapped individually with their respective image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Show some examples of test images to demonstrate how your pipeline is working. What did you do to optimize the performance of your classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"3.png\">\n",
    "\n",
    "<img src=\"4.png\">\n",
    "\n",
    "<img src=\"5.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier did good on the test images and were producing good results. YCrCb channel with all the HOG channeles were used in this case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Provide a link to your final video output. Your pipeline should perform reasonably well on the entire project video\n",
    "\n",
    "Here is the video output of the project_video:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<video controls src=\"project_output_3.mp4\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Describe how (and identify where in your code) you implemented some kind of filter for false positives and some method for combining overlapping bounding boxes."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def add_heat(heatmap, bbox_list):\n",
    "    # Iterate through list of bboxes\n",
    "    for box in bbox_list:\n",
    "        # Add += 1 for all pixels inside each bbox\n",
    "        # Assuming each \"box\" takes the form ((x1, y1), (x2, y2))\n",
    "        heatmap[box[0][1]:box[1][1], box[0][0]:box[1][0]] += 1\n",
    "\n",
    "    # Return updated heatmap\n",
    "    return heatmap\n",
    "\n",
    "def apply_threshold(heatmap, threshold):\n",
    "    # Zero out pixels below the threshold\n",
    "    heatmap[heatmap <= threshold] = 0\n",
    "    # Return thresholded map\n",
    "    return heatmap\n",
    "\n",
    "def draw_labeled_bboxes(img, labels, color=(0, 255, 0), thickness=5):\n",
    "    cars_bbox = []\n",
    "    # Iterate through all detected cars\n",
    "    for car_number in range(1, labels[1]+1):\n",
    "        # Find pixels with each car_number label value\n",
    "        nonzero = (labels[0] == car_number).nonzero()\n",
    "        # Identify x and y values of those pixels\n",
    "        nonzeroy = np.array(nonzero[0])\n",
    "        nonzerox = np.array(nonzero[1])\n",
    "        # Define a bounding box based on min/max x and y\n",
    "        bbox = ((np.min(nonzerox), np.min(nonzeroy)), (np.max(nonzerox), np.max(nonzeroy)))\n",
    "        # Draw the box on the image\n",
    "        cv2.rectangle(img, bbox[0], bbox[1], color, thickness)\n",
    "        cars_bbox.append(bbox)\n",
    "    # Return the image\n",
    "    return img, cars_bbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This snippet of the code can be found in the 3rd cell of the Trial_final_3.ipynb file. Even though I added the add_heat functionality in the main find_cars fucntions I just wanted to higlight it here. add_heat and apply_threshold helps in making blobs of detection of the object with scipy.ndimage.measurements.label() to label individual blob and then draw_labeled_bboxes generalizes all the overlaped blobs of heat map over frames and draws a bounding box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"output_images/3.png\">\n",
    "<img src=\"output_images/4.png\">\n",
    "<img src=\"output_images/5.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Briefly discuss any problems / issues you faced in your implementation of this project. Where will your pipeline likely fail? What could you do to make it more robust?\n",
    "\n",
    "I first started with a different approach of sliding window which took the search_windows and single_img_features functions and constructed windows with varying sizes for the far planes and larger for the nearer planes. That worked nicely for the static images but for the video without the use of tracking method between frames it was difficult to run on the video. It tool lot fine tuning so I steered my way to the find_cars method and that worked good with different scales. I then experimented different scales and decided on the scale of 1.5. I initially faced lot of issues with the incompatibilites of my function with the heatmap function and so decided to put in the find_cars function itselt and that made life easier. For video with only one scale(1.5) few frames were not detecting any car at all so I decided to add in one more scale to check if that solves the issue and luckily it did so I chose 1.5 and 1.0 scales for the video.\n",
    "\n",
    "My model isn't robust enough as it still is a jittery detection. And further I would like improve my pipeine so that I can achieve stable detection with no false positives. I checked out a fellow student's implementation and his implementation and the result of it was so James Bond or Iron Man type cool and I would really like to have my pipeline like that and so I have decided to work on it after submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
